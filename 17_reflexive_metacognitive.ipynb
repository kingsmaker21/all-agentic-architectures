{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-title",
      "metadata": {
        "id": "intro-title"
      },
      "source": [
        "# 📘 Agentic Architectures 17: Reflexive Metacognitive Agents\n",
        "\n",
        "Welcome to an in-depth implementation of one of the most sophisticated agentic patterns: the **Reflexive Metacognitive Agent**. This architecture endows an agent with a form of self-awareness, enabling it to reason about its own capabilities, confidence, and limitations before taking action.\n",
        "\n",
        "This goes a step beyond simple self-reflection. A metacognitive agent maintains an explicit **\"self-model\"**—a structured representation of its own knowledge, tools, and boundaries. When faced with a task, its first step is not to solve the problem, but to *analyze the problem in the context of its self-model*. It asks internal questions like:\n",
        "- \"Do I have sufficient knowledge to answer this confidently?\"\n",
        "- \"Is this topic within my designated area of expertise?\"\n",
        "- \"Do I have a specific tool that is required to answer this safely and accurately?\"\n",
        "- \"Is the user's query about a high-stakes topic where an error would be dangerous?\"\n",
        "\n",
        "Based on the answers, it chooses a strategy: reason directly, use a specialized tool, or—most importantly—**escalate to a human** when the task exceeds its known limits.\n",
        "\n",
        "To build a complex and powerful demonstration, we will create a **Medical Triage & Information Assistant**. This is a classic high-stakes scenario where the agent's ability to recognize its limitations is not just a feature, but a critical safety requirement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-definition",
      "metadata": {
        "id": "intro-definition"
      },
      "source": [
        "### Definition\n",
        "A **Reflexive Metacognitive Agent** is an agent that maintains and uses an explicit model of its own capabilities, knowledge boundaries, and confidence levels to select the most appropriate strategy for a given task. This self-modeling allows it to behave more safely and reliably, especially in domains where incorrect information is harmful.\n",
        "\n",
        "### High-level Workflow\n",
        "\n",
        "1.  **Perceive Task:** The agent receives a user request.\n",
        "2.  **Metacognitive Analysis (Self-Reflection):** The agent's core reasoning engine analyzes the request *against its own self-model*. It assesses its confidence, the relevance of its tools, and whether the query falls within its predefined operational domain.\n",
        "3.  **Strategy Selection:** Based on the analysis, the agent selects one of several strategies:\n",
        "    *   **Reason Directly:** For high-confidence, low-risk queries within its knowledge base.\n",
        "    *   **Use Tool:** When the query requires a specific capability the agent possesses via a tool.\n",
        "    *   **Escalate/Refuse:** For low-confidence, high-risk, or out-of-scope queries.\n",
        "4.  **Execute Strategy:** The chosen path is executed.\n",
        "5.  **Respond:** The agent provides the result, which could be a direct answer, a tool-augmented answer, or a safe refusal with instructions to consult an expert.\n",
        "\n",
        "### When to Use / Applications\n",
        "*   **High-Stakes Advisory Systems:** Any system providing information in domains like healthcare, law, or finance, where an agent must be able to say \"I don't know\" or \"You should consult a professional.\"\n",
        "*   **Autonomous Systems:** A robot that must assess its own ability to perform a physical task safely before attempting it.\n",
        "*   **Complex Tool Orchestrators:** An agent that must choose the right API from a vast library, understanding that some APIs are more dangerous or costly than others.\n",
        "\n",
        "### Strengths & Weaknesses\n",
        "*   **Strengths:**\n",
        "    *   **Enhanced Safety and Reliability:** The primary benefit. The agent is explicitly designed to avoid making confident assertions in areas where it is not an expert.\n",
        "    *   **Improved Decision Making:** Leads to more robust behavior by forcing a deliberate choice of strategy instead of naive, direct attempts.\n",
        "*   **Weaknesses:**\n",
        "    *   **Complexity of Self-Model:** Defining and maintaining an accurate self-model can be complex.\n",
        "    *   **Metacognitive Overhead:** The initial analysis step adds latency and computational cost to every request."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase0-title",
      "metadata": {
        "id": "phase0-title"
      },
      "source": [
        "## Phase 0: Foundation & Setup\n",
        "\n",
        "Standard setup of libraries and environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "install-libs",
      "metadata": {
        "id": "install-libs",
        "outputId": "f1264924-5fa5-4384-d57b-8676cba6f31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.23.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-nebius langchain langgraph rich python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "import-and-keys",
      "metadata": {
        "id": "import-and-keys",
        "outputId": "ab03b7ed-a23c-41b9-ee21-fe9c1e4dd304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables loaded and tracing is set up.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Pydantic for data modeling\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangChain components\n",
        "from langchain_nebius import ChatNebius\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# LangGraph components\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# For pretty printing\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "from rich.panel import Panel\n",
        "\n",
        "# --- API Key and Tracing Setup ---\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agentic Architecture - Metacognitive Agent (Nebius)\"\n",
        "\n",
        "required_vars = [\"NEBIUS_API_KEY\", \"LANGCHAIN_API_KEY\"]\n",
        "for var in required_vars:\n",
        "    if var not in os.environ:\n",
        "        print(f\"Warning: Environment variable {var} not set.\")\n",
        "\n",
        "print(\"Environment variables loaded and tracing is set up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase1-title",
      "metadata": {
        "id": "phase1-title"
      },
      "source": [
        "## Phase 1: Defining the Agent's Self-Model and Tools\n",
        "\n",
        "This is the foundation of the agent's self-awareness. We'll create a structured `AgentSelfModel` and a specialized tool. This model is not just a prompt; it's a configuration object that will be passed into the agent's reasoning loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "environment-setup-code",
      "metadata": {
        "id": "environment-setup-code",
        "outputId": "08148dfd-9890-4971-9acd-c2ef72c7070d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Self-Model and Tools defined successfully.\n"
          ]
        }
      ],
      "source": [
        "console = Console()\n",
        "\n",
        "# --- The Agent's Self-Model ---\n",
        "class AgentSelfModel(BaseModel):\n",
        "    \"\"\"A structured representation of the agent's capabilities and limitations.\"\"\"\n",
        "    name: str\n",
        "    role: str\n",
        "    # The agent's explicit knowledge boundaries\n",
        "    knowledge_domain: List[str] = Field(description=\"List of topics the agent is knowledgeable about.\")\n",
        "    # The agent's available tools\n",
        "    available_tools: List[str] = Field(description=\"List of tools the agent can use.\")\n",
        "    confidence_threshold: float = Field(description=\"The confidence level (0-1) below which the agent must escalate.\", default=0.6)\n",
        "\n",
        "# Instantiate the self-model for our Medical Triage Agent\n",
        "medical_agent_model = AgentSelfModel(\n",
        "    name=\"TriageBot-3000\",\n",
        "    role=\"A helpful AI assistant for providing preliminary medical information.\",\n",
        "    knowledge_domain=[\"common_cold\", \"influenza\", \"allergies\", \"headaches\", \"basic_first_aid\"],\n",
        "    available_tools=[\"drug_interaction_checker\"]\n",
        ")\n",
        "\n",
        "# --- Specialist Tools ---\n",
        "class DrugInteractionChecker:\n",
        "    \"\"\"A mock tool to check for drug interactions.\"\"\"\n",
        "    def check(self, drug_a: str, drug_b: str) -> str:\n",
        "        \"\"\"Checks for interactions between two drugs.\"\"\"\n",
        "        # In a real system, this would query a medical database.\n",
        "        known_interactions = {\n",
        "            frozenset([\"ibuprofen\", \"lisinopril\"]): \"Moderate risk: Ibuprofen may reduce the blood pressure-lowering effects of lisinopril. Monitor blood pressure.\",\n",
        "            frozenset([\"aspirin\", \"warfarin\"]): \"High risk: Increased risk of bleeding. This combination should be avoided unless directed by a doctor.\"\n",
        "        }\n",
        "        interaction = known_interactions.get(frozenset([drug_a.lower(), drug_b.lower()]))\n",
        "        if interaction:\n",
        "            return f\"Interaction Found: {interaction}\"\n",
        "        return \"No known significant interactions found. However, always consult a pharmacist or doctor.\"\n",
        "\n",
        "drug_tool = DrugInteractionChecker()\n",
        "print(\"Agent Self-Model and Tools defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace line: llm = ChatNebius(model=\"mistralai/Mixtral-8x22B-Instruct-v0.1\", temperature=0)\n",
        "\n",
        "# With this:\n",
        "MODEL_FALLBACKS = [\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"mixtral-8x7b-instruct\",\n",
        "    \"mistral-medium\",\n",
        "]\n",
        "\n",
        "llm = None\n",
        "for model_name in MODEL_FALLBACKS:\n",
        "    try:\n",
        "        llm = ChatNebius(model=model_name, temperature=0)\n",
        "        test = llm.invoke(\"test\")  # Verify it works\n",
        "        print(f\"✅ Using model: {model_name}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {model_name} failed\")\n",
        "        continue\n",
        "\n",
        "if llm is None:\n",
        "    print(\"⚠️ All Nebius models failed. Switching to OpenAI fallback...\")\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "0EIBRlMjHkDA"
      },
      "id": "0EIBRlMjHkDA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "phase2-title",
      "metadata": {
        "id": "phase2-title"
      },
      "source": [
        "## Phase 2: Building the Metacognitive Agent with LangGraph\n",
        "\n",
        "This is where the magic happens. We'll build a graph where the very first step is the **metacognitive analysis**. This node will use a powerful, detailed prompt to make the agent reason about itself. A conditional router will then direct the flow based on the chosen strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "agent-build-code",
      "metadata": {
        "id": "agent-build-code",
        "outputId": "63e29b77-2d3a-4fe9-f569-6a31c192504a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reflexive Metacognitive Agent graph compiled successfully.\n"
          ]
        }
      ],
      "source": [
        "#llm = ChatNebius(model=\"mistralai/Mixtral-8x22B-Instruct-v0.1\", temperature=0)\n",
        "\n",
        "# Pydantic Models for structured outputs\n",
        "class MetacognitiveAnalysis(BaseModel):\n",
        "    \"\"\"The agent's self-analysis of a query.\"\"\"\n",
        "    confidence: float = Field(description=\"A score from 0.0 to 1.0 representing the agent's confidence in its ability to answer safely and accurately.\")\n",
        "    strategy: str = Field(description=\"The chosen strategy. Must be one of: 'reason_directly', 'use_tool', 'escalate'.\")\n",
        "    reasoning: str = Field(description=\"A brief justification for the chosen confidence and strategy.\")\n",
        "    tool_to_use: Optional[str] = Field(description=\"If strategy is 'use_tool', the name of the tool to use.\", default=None)\n",
        "    tool_args: Optional[Dict[str, Any]] = Field(description=\"If strategy is 'use_tool', the arguments for the tool.\", default=None)\n",
        "\n",
        "# LangGraph State\n",
        "class AgentState(TypedDict):\n",
        "    user_query: str\n",
        "    self_model: AgentSelfModel\n",
        "    metacognitive_analysis: Optional[MetacognitiveAnalysis]\n",
        "    tool_output: Optional[str]\n",
        "    final_response: str\n",
        "\n",
        "# Graph Nodes\n",
        "def metacognitive_analysis_node(state: AgentState) -> Dict[str, Any]:\n",
        "    console.print(Panel(\"🤔 Agent is performing metacognitive analysis...\", title=\"[yellow]Step: Self-Reflection[/yellow]\"))\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a metacognitive reasoning engine for an AI assistant. Your task is to analyze a user's query in the context of the agent's own capabilities and limitations (its 'self-model').\n",
        "        Your primary directive is **SAFETY**. You must determine the safest and most appropriate strategy for handling the query.\n",
        "\n",
        "        **Agent's Self-Model:**\n",
        "        - Name: {agent_name}\n",
        "        - Role: {agent_role}\n",
        "        - Knowledge Domain: {knowledge_domain}\n",
        "        - Available Tools: {available_tools}\n",
        "\n",
        "        **Strategy Rules:**\n",
        "        1.  **escalate:** Choose this strategy if the query involves a potential medical emergency (e.g., chest pain, difficulty breathing, severe injury, broken bones), is outside the agent's knowledge domain, or if you have any doubt about providing a safe answer. **WHEN IN DOUBT, ESCALATE.**\n",
        "        2.  **use_tool:** Choose this strategy if the query explicitly or implicitly requires one of the available tools. For example, a question about drug interactions requires the 'drug_interaction_checker'.\n",
        "        3.  **reason_directly:** Choose this strategy ONLY if you are highly confident the query is a simple, low-risk question that falls squarely within the agent's knowledge domain.\n",
        "\n",
        "        Analyze the user query below and provide your metacognitive analysis in the required format.\n",
        "\n",
        "        **User Query:** \"{query}\"\"\"\n",
        "    )\n",
        "    chain = prompt | llm.with_structured_output(MetacognitiveAnalysis)\n",
        "    analysis = chain.invoke({\n",
        "        \"query\": state['user_query'],\n",
        "        \"agent_name\": state['self_model'].name,\n",
        "        \"agent_role\": state['self_model'].role,\n",
        "        \"knowledge_domain\": \", \".join(state['self_model'].knowledge_domain),\n",
        "        \"available_tools\": \", \".join(state['self_model'].available_tools),\n",
        "    })\n",
        "    console.print(Panel(f\"[bold]Confidence:[/bold] {analysis.confidence:.2f}\\n[bold]Strategy:[/bold] {analysis.strategy}\\n[bold]Reasoning:[/bold] {analysis.reasoning}\", title=\"Metacognitive Analysis Result\"))\n",
        "    return {\"metacognitive_analysis\": analysis}\n",
        "\n",
        "def reason_directly_node(state: AgentState) -> Dict[str, Any]:\n",
        "    console.print(Panel(\"✅ Confident in direct answer. Generating response...\", title=\"[green]Strategy: Reason Directly[/green]\"))\n",
        "    prompt = ChatPromptTemplate.from_template(\"You are {agent_role}. Provide a helpful, non-prescriptive answer to the user's query. Remind the user that you are not a doctor.\\n\\nQuery: {query}\")\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"agent_role\": state['self_model'].role, \"query\": state['user_query']}).content\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "def call_tool_node(state: AgentState) -> Dict[str, Any]:\n",
        "    console.print(Panel(f\"🛠️ Confidence requires tool use. Calling `{state['metacognitive_analysis'].tool_to_use}`...\", title=\"[cyan]Strategy: Use Tool[/cyan]\"))\n",
        "    analysis = state['metacognitive_analysis']\n",
        "    if analysis.tool_to_use == 'drug_interaction_checker':\n",
        "        tool_output = drug_tool.check(**analysis.tool_args)\n",
        "        return {\"tool_output\": tool_output}\n",
        "    return {\"tool_output\": \"Error: Tool not found.\"}\n",
        "\n",
        "def synthesize_tool_response_node(state: AgentState) -> Dict[str, Any]:\n",
        "    console.print(Panel(\"📝 Synthesizing final response from tool output...\", title=\"[cyan]Step: Synthesize[/cyan]\"))\n",
        "    prompt = ChatPromptTemplate.from_template(\"You are {agent_role}. You have used a tool to get specific information. Now, present this information to the user in a clear and helpful way. ALWAYS include a disclaimer to consult a healthcare professional.\\n\\nOriginal Query: {query}\\nTool Output: {tool_output}\")\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"agent_role\": state['self_model'].role, \"query\": state['user_query'], \"tool_output\": state['tool_output']}).content\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "def escalate_to_human_node(state: AgentState) -> Dict[str, Any]:\n",
        "    console.print(Panel(\"🚨 Low confidence or high risk detected. Escalating to human.\", title=\"[bold red]Strategy: Escalate[/bold red]\"))\n",
        "    response = \"I am an AI assistant and not qualified to provide information on this topic. This query is outside my knowledge domain or involves potentially serious symptoms. **Please consult a qualified medical professional immediately.**\"\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "# Conditional Edge\n",
        "def route_strategy(state: AgentState) -> str:\n",
        "    return state[\"metacognitive_analysis\"].strategy\n",
        "\n",
        "# Build the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"analyze\", metacognitive_analysis_node)\n",
        "workflow.add_node(\"reason\", reason_directly_node)\n",
        "workflow.add_node(\"call_tool\", call_tool_node)\n",
        "workflow.add_node(\"synthesize\", synthesize_tool_response_node)\n",
        "workflow.add_node(\"escalate\", escalate_to_human_node)\n",
        "\n",
        "workflow.set_entry_point(\"analyze\")\n",
        "workflow.add_conditional_edges(\"analyze\", route_strategy, {\n",
        "    \"reason_directly\": \"reason\",\n",
        "    \"use_tool\": \"call_tool\",\n",
        "    \"escalate\": \"escalate\"\n",
        "})\n",
        "workflow.add_edge(\"call_tool\", \"synthesize\")\n",
        "workflow.add_edge(\"reason\", END)\n",
        "workflow.add_edge(\"synthesize\", END)\n",
        "workflow.add_edge(\"escalate\", END)\n",
        "\n",
        "metacognitive_agent = workflow.compile()\n",
        "print(\"Reflexive Metacognitive Agent graph compiled successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase3-title",
      "metadata": {
        "id": "phase3-title"
      },
      "source": [
        "## Phase 3: Demonstration & Analysis\n",
        "\n",
        "Now we will test the agent with a series of increasingly difficult and high-stakes queries. We will observe how the metacognitive analysis correctly routes each query down the appropriate path, demonstrating the system's safety and self-awareness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "demo-code",
      "metadata": {
        "id": "demo-code",
        "outputId": "82757282-8e6b-410b-9ecf-e0ba8498f501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--- Test \u001b[1;36m1\u001b[0m: Simple, In-Scope, Low-Risk Query ---\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Test <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Simple, In-Scope, Low-Risk Query ---\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭───────────────────────────────────────────── \u001b[33mStep: Self-Reflection\u001b[0m ─────────────────────────────────────────────╮\n",
              "│ 🤔 Agent is performing metacognitive analysis...                                                                │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────────────────── <span style=\"color: #808000; text-decoration-color: #808000\">Step: Self-Reflection</span> ─────────────────────────────────────────────╮\n",
              "│ 🤔 Agent is performing metacognitive analysis...                                                                │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'detail': 'The model `mistralai/Mixtral-8x22B-Instruct-v0.1` does not exist.'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-477687572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Test 1: Simple, should be answered directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Test 1: Simple, In-Scope, Low-Risk Query ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What are the symptoms of a common cold?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Test 2: Requires the specific tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-477687572.py\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"user_query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"self_model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmedical_agent_model\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetacognitive_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2655\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-209590580.py\u001b[0m in \u001b[0;36mmetacognitive_analysis_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_structured_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMetacognitiveAnalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     analysis = chain.invoke({\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m\"agent_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3246\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3247\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5709\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5710\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5711\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5712\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5713\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         return cast(\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1024\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 results.append(\n\u001b[0;32m--> 842\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    843\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         if (\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                 )\n\u001b[1;32m   1171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'detail': 'The model `mistralai/Mixtral-8x22B-Instruct-v0.1` does not exist.'}"
          ]
        }
      ],
      "source": [
        "def run_agent(query: str):\n",
        "    initial_state = {\"user_query\": query, \"self_model\": medical_agent_model}\n",
        "    result = metacognitive_agent.invoke(initial_state)\n",
        "    console.print(Markdown(result['final_response']))\n",
        "\n",
        "# Test 1: Simple, should be answered directly\n",
        "console.print(\"--- Test 1: Simple, In-Scope, Low-Risk Query ---\")\n",
        "run_agent(\"What are the symptoms of a common cold?\")\n",
        "\n",
        "# Test 2: Requires the specific tool\n",
        "console.print(\"\\n--- Test 2: Specific Query Requiring a Tool ---\")\n",
        "run_agent(\"Is it safe to take Ibuprofen if I am also taking Lisinopril?\")\n",
        "\n",
        "# Test 3: High-stakes, should be escalated immediately\n",
        "console.print(\"\\n--- Test 3: High-Stakes, Emergency Query ---\")\n",
        "run_agent(\"I have a crushing pain in my chest and my left arm feels numb, what should I do?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analysis-markdown",
      "metadata": {
        "id": "analysis-markdown"
      },
      "source": [
        "### Analysis of the Results\n",
        "\n",
        "The demonstration is a powerful illustration of the safety and reliability this architecture provides:\n",
        "\n",
        "1.  **Correctly Scoped Answer:** For the 'common cold' query, the metacognitive analysis correctly identified it as a low-risk topic within its knowledge domain. It set a high confidence score and chose the `reason_directly` strategy, providing a helpful but properly caveated answer.\n",
        "\n",
        "2.  **Correct Tool Use:** For the drug interaction question, the analysis recognized the need for a specific capability. It correctly identified that the `drug_interaction_checker` tool was required, set a high confidence *in its ability to use the tool*, and selected the `use_tool` strategy. The final response was a safe, synthesized summary of the tool's output.\n",
        "\n",
        "3.  **Critical Safety Escalation:** This is the most important result. A naive agent might have tried to answer the 'chest pain' query by searching the web for causes, potentially providing dangerous and misleading information. Our metacognitive agent, guided by its primary directive of safety, immediately recognized the signs of a medical emergency. The metacognitive analysis assigned a very low confidence score and correctly chose the `escalate` strategy. The final output was not an answer, but a safe, responsible refusal and a directive to seek professional help. It correctly identified the limits of its own competence.\n",
        "\n",
        "This workflow proves that by forcing the agent to reason about itself *before* reasoning about the problem, we can build a powerful layer of safety and reliability into its operation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this detailed notebook, we have implemented a **Reflexive Metacognitive Agent**, a sophisticated architecture that prioritizes safety and reliability by endowing an agent with self-awareness. By building an explicit `self-model` and forcing a metacognitive analysis as the first step of any task, we have created a system that understands its own boundaries.\n",
        "\n",
        "The key innovation is the shift in the agent's initial goal from \"How do I answer this?\" to \"*Should* I answer this, and if so, how?\" This introspective step allows the agent to dynamically choose the safest and most appropriate strategy—be it direct reasoning, specialized tool use, or crucial escalation to a human expert.\n",
        "\n",
        "This architecture is more than just a technique; it's a design philosophy. It is absolutely essential for creating responsible AI agents that can be trusted to operate in high-stakes, real-world domains where knowing what you *don't* know is just as important as what you do."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}