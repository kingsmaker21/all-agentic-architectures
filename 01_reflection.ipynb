{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-title",
      "metadata": {
        "id": "intro-title"
      },
      "source": [
        "# ðŸ“˜ Agentic Architectures 1: Reflection\n",
        "\n",
        "Welcome to the first notebook in our deep dive into the 21 key agentic architectures. We begin with one of the most fundamental and powerful patterns: **Reflection**.\n",
        "\n",
        "This pattern elevates a Large Language Model (LLM) from a simple, single-pass generator into a more deliberate and robust reasoner. Instead of just providing the first answer it comes up with, a reflective agent takes a step back to critique, analyze, and refine its own work. This iterative process of self-improvement is a cornerstone of building more reliable and higher-quality AI systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-definition",
      "metadata": {
        "id": "intro-definition"
      },
      "source": [
        "### Definition\n",
        "The **Reflection** architecture involves an agent critiquing and revising its own output before returning a final answer. Instead of a single-pass generation, it engages in a multi-step internal monologue: produce, evaluate, and improve. This mimics the human process of drafting, reviewing, and editing to catch errors and enhance quality.\n",
        "\n",
        "### High-level Workflow\n",
        "\n",
        "1.  **Generate:** The agent produces an initial draft or solution based on the user's prompt.\n",
        "2.  **Critique:** The agent then switches roles to become a critic. It asks itself questions like: *\"What could be wrong with this answer?\"*, *\"What is missing?\"*, *\"Is this solution optimal?\"*, or *\"Are there any logical flaws or bugs?\"*.\n",
        "3.  **Refine:** Using the insights from its self-critique, the agent generates a final, improved version of the output.\n",
        "\n",
        "### When to Use / Applications\n",
        "*   **Code Generation:** The initial code might have bugs, be inefficient, or lack comments. Reflection allows the agent to act as its own code reviewer, catching errors and improving style before presenting the final script.\n",
        "*   **Complex Summarization:** When summarizing dense documents, a first pass might miss nuances or omit key details. A reflection step helps ensure the summary is comprehensive and accurate.\n",
        "*   **Creative Writing & Content Creation:** The first draft of an email, blog post, or story can always be improved. Reflection allows the agent to refine its tone, clarity, and impact.\n",
        "\n",
        "### Strengths & Weaknesses\n",
        "*   **Strengths:**\n",
        "    *   **Improved Quality:** Directly addresses and corrects errors, leading to more accurate, robust, and well-reasoned outputs.\n",
        "    *   **Low Overhead:** It's a conceptually simple pattern that can be implemented with a single LLM and doesn't require complex external tools.\n",
        "*   **Weaknesses:**\n",
        "    *   **Self-Bias:** The agent is still limited by its own knowledge and biases. If it doesn't know a better way to solve a problem, it can't critique its way to a better solution. It can fix flaws it recognizes but can't invent knowledge it lacks.\n",
        "    *   **Increased Latency & Cost:** The process involves at least two LLM calls (generation + critique/refinement), making it slower and more expensive than a single-pass approach."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase0-title",
      "metadata": {
        "id": "phase0-title"
      },
      "source": [
        "## Phase 0: Foundation & Setup\n",
        "\n",
        "Before we build our reflective agent, we need to set up our environment. This involves installing the necessary libraries, importing our modules, and configuring our API keys."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-what",
      "metadata": {
        "id": "setup-what"
      },
      "source": [
        "### Step 0.1: Installing Core Libraries\n",
        "\n",
        "**What we are going to do:**\n",
        "We'll install the essential Python libraries for this project. The `langchain-nebius` package provides access to Nebius AI Studio models, `langchain` and `langgraph` will provide the core orchestration framework, `python-dotenv` will manage our API keys, and `rich` will help us print the outputs nicely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "install-libs",
      "metadata": {
        "id": "install-libs"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain-nebius langchain langgraph rich python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEBIUS_API_KEY=\"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwMjY1NTQyMTc2MTIzMzM4MzA3MSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkxNzA1OTcyNCwidXVpZCI6IjAxOTlhMzM0LTEyMWYtNzg3My04YjI3LTc5Njg1NjcwMWE1NSIsIm5hbWUiOiJ0cnN0IiwiZXhwaXJlc19hdCI6IjIwMzAtMTAtMDFUMDQ6MzU6MjQrMDAwMCJ9.uXqXwIsnCOZTGTZVVBU9fHIxamPwlDaMahFdVJl0GT0\"\n",
        "LANGCHAIN_API_KEY=\"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\""
      ],
      "metadata": {
        "id": "wSBJ8LsKOSdH"
      },
      "id": "wSBJ8LsKOSdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "imports-what",
      "metadata": {
        "id": "imports-what"
      },
      "source": [
        "### Step 0.2: Importing Libraries and Setting Up Keys\n",
        "\n",
        "**What we are going to do:**\n",
        "Now we'll import all the necessary components from our installed libraries. We'll use the `python-dotenv` library to securely load our Nebius API key from a local `.env` file. We will also set up LangSmith for tracing, which is invaluable for debugging multi-step agentic workflows.\n",
        "\n",
        "**Action Required:** You must create a file named `.env` in the same directory as this notebook and add your keys to it, like this:\n",
        "```\n",
        "NEBIUS_API_KEY=\"your_nebius_api_key_here\"\n",
        "LANGCHAIN_API_KEY=\"your_langsmith_api_key_here\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "import-and-keys",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "import-and-keys",
        "outputId": "4f1d37ea-8ac4-4177-cf48-df4c2944e51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEBIUS_API_KEY not found. Please create a .env file and set it.\n",
            "LANGCHAIN_API_KEY not found. Please create a .env file and set it for tracing.\n",
            "Environment variables loaded and tracing is set up.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, TypedDict, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Nebius and LangChain components\n",
        "from langchain_nebius import ChatNebius\n",
        "from pydantic import BaseModel, Field # Corrected import for Pydantic v2\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# For pretty printing\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "from rich.syntax import Syntax\n",
        "\n",
        "# --- API Key and Tracing Setup ---\n",
        "load_dotenv()\n",
        "\n",
        "# Set up LangSmith tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agentic Architecture - Reflection (Nebius)\"\n",
        "\n",
        "# Check that the keys are set\n",
        "if not os.environ.get(\"NEBIUS_API_KEY\"):\n",
        "    print(\"NEBIUS_API_KEY not found. Please create a .env file and set it.\")\n",
        "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
        "    print(\"LANGCHAIN_API_KEY not found. Please create a .env file and set it for tracing.\")\n",
        "\n",
        "print(\"Environment variables loaded and tracing is set up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase1-title",
      "metadata": {
        "id": "phase1-title"
      },
      "source": [
        "## Phase 1: Building the Core Components of Reflection\n",
        "\n",
        "A robust reflection architecture is more than just a simple prompt. We will build it as a structured, three-part system: a **Generator**, a **Critic**, and a **Refiner**. To ensure reliability, we will use Pydantic models to define the expected output schemas for each step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pydantic-what",
      "metadata": {
        "id": "pydantic-what"
      },
      "source": [
        "### Step 1.1: Defining the Data Schemas with Pydantic\n",
        "\n",
        "**What we are going to do:**\n",
        "We'll define Pydantic models that act as a contract for our LLM. This tells the LLM exactly what structure its output should have, which is critical for a multi-step process where the output of one step becomes the input for the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pydantic-models",
      "metadata": {
        "id": "pydantic-models"
      },
      "outputs": [],
      "source": [
        "class DraftCode(BaseModel):\n",
        "    \"\"\"Schema for the initial code draft generated by the agent.\"\"\"\n",
        "    code: str = Field(description=\"The Python code generated to solve the user's request.\")\n",
        "    explanation: str = Field(description=\"A brief explanation of how the code works.\")\n",
        "\n",
        "class Critique(BaseModel):\n",
        "    \"\"\"Schema for the self-critique of the generated code.\"\"\"\n",
        "    has_errors: bool = Field(description=\"Does the code have any potential bugs or logical errors?\")\n",
        "    is_efficient: bool = Field(description=\"Is the code written in an efficient and optimal way?\")\n",
        "    suggested_improvements: List[str] = Field(description=\"Specific, actionable suggestions for improving the code.\")\n",
        "    critique_summary: str = Field(description=\"A summary of the critique.\")\n",
        "\n",
        "class RefinedCode(BaseModel):\n",
        "    \"\"\"Schema for the final, refined code after incorporating the critique.\"\"\"\n",
        "    refined_code: str = Field(description=\"The final, improved Python code.\")\n",
        "    refinement_summary: str = Field(description=\"A summary of the changes made based on the critique.\")\n",
        "\n",
        "print(\"Pydantic models for Draft, Critique, and RefinedCode have been defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pydantic-discuss",
      "metadata": {
        "id": "pydantic-discuss"
      },
      "source": [
        "**Discussion of the Output:**\n",
        "We have successfully defined our data structures. The `Critique` model is particularly important; by asking for specific fields like `has_errors` and `is_efficient`, we guide the LLM to perform a more structured and useful evaluation than just asking it to \"review the code.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llm-what",
      "metadata": {
        "id": "llm-what"
      },
      "source": [
        "### Step 1.2: Initializing the Nebius LLM and the Console\n",
        "\n",
        "**What we are going to do:**\n",
        "We will initialize the Nebius language model that will power all three roles (Generator, Critic, and Refiner). We'll use a powerful model like `meta-llama/Meta-Llama-3.1-8B-Instruct` to ensure high-quality reasoning for all steps. We'll also set up our `rich` console for clean, formatted output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "llm-init",
      "metadata": {
        "id": "llm-init"
      },
      "outputs": [],
      "source": [
        "# Use a powerful Nebius model for generation and critique\n",
        "llm = ChatNebius(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", temperature=0.2)\n",
        "\n",
        "# Initialize console for pretty printing\n",
        "console = Console()\n",
        "\n",
        "print(\"Nebius LLM and Console are initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generator-what",
      "metadata": {
        "id": "generator-what"
      },
      "source": [
        "### Step 1.3: Creating the Generator Node\n",
        "\n",
        "**What we are going to do:**\n",
        "This node's only job is to take the user's request and produce the first draft. We will bind our `DraftCode` Pydantic model to the Nebius LLM to ensure its output is structured correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generator-node-code",
      "metadata": {
        "id": "generator-node-code"
      },
      "outputs": [],
      "source": [
        "def generator_node(state):\n",
        "    \"\"\"Generates the initial draft of the code.\"\"\"\n",
        "    console.print(\"--- 1. Generating Initial Draft ---\")\n",
        "    generator_llm = llm.with_structured_output(DraftCode)\n",
        "\n",
        "    prompt = f\"\"\"You are an expert Python programmer. Write a Python function to solve the following request.\n",
        "    Provide a simple, clear implementation and an explanation.\n",
        "\n",
        "    Request: {state['user_request']}\n",
        "    \"\"\"\n",
        "\n",
        "    draft = generator_llm.invoke(prompt)\n",
        "    return {\"draft\": draft.model_dump()} # Corrected: use .model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "critic-what",
      "metadata": {
        "id": "critic-what"
      },
      "source": [
        "### Step 1.4: Creating the Critic Node\n",
        "\n",
        "**What we are going to do:**\n",
        "This is the core of the reflection process. The Critic node takes the initial draft, analyzes it for flaws, and produces a structured critique using our `Critique` Pydantic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "critic-node-code",
      "metadata": {
        "id": "critic-node-code"
      },
      "outputs": [],
      "source": [
        "def critic_node(state):\n",
        "    \"\"\"Critiques the generated code for errors and inefficiencies.\"\"\"\n",
        "    console.print(\"--- 2. Critiquing Draft ---\")\n",
        "    critic_llm = llm.with_structured_output(Critique)\n",
        "\n",
        "    code_to_critique = state['draft']['code']\n",
        "\n",
        "    prompt = f\"\"\"You are an expert code reviewer and senior Python developer. Your task is to perform a thorough critique of the following code.\n",
        "\n",
        "    Analyze the code for:\n",
        "    1.  **Bugs and Errors:** Are there any potential runtime errors, logical flaws, or edge cases that are not handled?\n",
        "    2.  **Efficiency and Best Practices:** Is this the most efficient way to solve the problem? Does it follow standard Python conventions (PEP 8)?\n",
        "\n",
        "    Provide a structured critique with specific, actionable suggestions.\n",
        "\n",
        "    Code to Review:\n",
        "    ```python\n",
        "    {code_to_critique}\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    critique = critic_llm.invoke(prompt)\n",
        "    return {\"critique\": critique.model_dump()} # Corrected: use .model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "refiner-what",
      "metadata": {
        "id": "refiner-what"
      },
      "source": [
        "### Step 1.5: Creating the Refiner Node\n",
        "\n",
        "**What we are going to do:**\n",
        "The final step in our logic is the Refiner. This node receives both the original draft and the structured critique and is tasked with writing the final, improved version of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "refiner-node-code",
      "metadata": {
        "id": "refiner-node-code"
      },
      "outputs": [],
      "source": [
        "def refiner_node(state):\n",
        "    \"\"\"Refines the code based on the critique.\"\"\"\n",
        "    console.print(\"--- 3. Refining Code ---\")\n",
        "    refiner_llm = llm.with_structured_output(RefinedCode)\n",
        "\n",
        "    draft_code = state['draft']['code']\n",
        "    critique_suggestions = json.dumps(state['critique'], indent=2)\n",
        "\n",
        "    prompt = f\"\"\"You are an expert Python programmer tasked with refining a piece of code based on a critique.\n",
        "\n",
        "    Your goal is to rewrite the original code, implementing all the suggested improvements from the critique.\n",
        "\n",
        "    **Original Code:**\n",
        "    ```python\n",
        "    {draft_code}\n",
        "    ```\n",
        "\n",
        "    **Critique and Suggestions:**\n",
        "    {critique_suggestions}\n",
        "\n",
        "    Please provide the final, refined code and a summary of the changes you made.\n",
        "    \"\"\"\n",
        "\n",
        "    refined_code = refiner_llm.invoke(prompt)\n",
        "    return {\"refined_code\": refined_code.model_dump()} # Corrected: use .model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase1-discuss",
      "metadata": {
        "id": "phase1-discuss"
      },
      "source": [
        "**Discussion of Phase 1:**\n",
        "We have now created the three core logical components of our reflective agent. Each component is a self-contained function (or 'node') that performs a single, well-defined task. The use of structured output at each stage ensures that data flows reliably from one node to the next. Now, we are ready to orchestrate this workflow using LangGraph."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase2-title",
      "metadata": {
        "id": "phase2-title"
      },
      "source": [
        "## Phase 2: Orchestrating the Reflection Workflow with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graph-state-what",
      "metadata": {
        "id": "graph-state-what"
      },
      "source": [
        "### Step 2.1: Defining the Graph State\n",
        "\n",
        "**What we are going to do:**\n",
        "The 'state' is the memory of our graph. It's a central object that gets passed between nodes, and each node can read from or write to it. We will define a `ReflectionState` using Python's `TypedDict` to hold all the pieces of our workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "graph-state-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "graph-state-code",
        "outputId": "e9d59c80-c2c2-41a9-8ca3-6a3fbeb2f22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReflectionState TypedDict defined.\n"
          ]
        }
      ],
      "source": [
        "class ReflectionState(TypedDict):\n",
        "    \"\"\"Represents the state of our reflection graph.\"\"\"\n",
        "    user_request: str\n",
        "    draft: Optional[dict]\n",
        "    critique: Optional[dict]\n",
        "    refined_code: Optional[dict]\n",
        "\n",
        "print(\"ReflectionState TypedDict defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graph-build-what",
      "metadata": {
        "id": "graph-build-what"
      },
      "source": [
        "### Step 2.2: Building and Visualizing the Graph\n",
        "\n",
        "**What we are going to do:**\n",
        "Now we will assemble our nodes into a coherent workflow using `StateGraph`. For this reflection pattern, the workflow is a simple linear sequence: **Generate â†’ Critique â†’ Refine**. We will define this flow and then compile and visualize the graph to confirm its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "graph-build-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "graph-build-code",
        "outputId": "6c24e14d-713a-43ef-e751-8e69681e83c7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'StateGraph' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1435538206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReflectionState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Add the nodes to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"critic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StateGraph' is not defined"
          ]
        }
      ],
      "source": [
        "graph_builder = StateGraph(ReflectionState)\n",
        "\n",
        "# Add the nodes to the graph\n",
        "graph_builder.add_node(\"generator\", generator_node)\n",
        "graph_builder.add_node(\"critic\", critic_node)\n",
        "graph_builder.add_node(\"refiner\", refiner_node)\n",
        "\n",
        "# Define the workflow edges\n",
        "graph_builder.set_entry_point(\"generator\")\n",
        "graph_builder.add_edge(\"generator\", \"critic\")\n",
        "graph_builder.add_edge(\"critic\", \"refiner\")\n",
        "graph_builder.add_edge(\"refiner\", END)\n",
        "\n",
        "# Compile the graph\n",
        "reflection_app = graph_builder.compile()\n",
        "\n",
        "print(\"Reflection graph compiled successfully!\")\n",
        "\n",
        "# Visualize the graph\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    png_image = reflection_app.get_graph().draw_png()\n",
        "    display(Image(png_image))\n",
        "except Exception as e:\n",
        "    print(f\"Graph visualization failed: {e}. Please ensure pygraphviz is installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graph-build-discuss",
      "metadata": {
        "id": "graph-build-discuss"
      },
      "source": [
        "**Discussion of the Output:**\n",
        "The graph has been successfully compiled. The visualization confirms our intended linear workflow. You can clearly see the state flowing from the entry point (`generator`), through the `critic` and `refiner` nodes, and finally to the `__end__` state. This simple but powerful structure is now ready for execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase3-title",
      "metadata": {
        "id": "phase3-title"
      },
      "source": [
        "## Phase 3: End-to-End Execution and Evaluation\n",
        "\n",
        "With our graph compiled, it's time to see the reflection pattern in action. We'll give it a coding task where a naive first attempt is likely to be suboptimal, making it a perfect test case for self-critique and refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "execution-what",
      "metadata": {
        "id": "execution-what"
      },
      "source": [
        "### Step 3.1: Running the Full Reflection Workflow\n",
        "\n",
        "**What we are going to do:**\n",
        "We will invoke our compiled LangGraph application with a request to write a function to find the nth Fibonacci number. We will stream the results and properly accumulate the full state so we can inspect all intermediate steps at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution-code",
      "metadata": {
        "id": "execution-code"
      },
      "outputs": [],
      "source": [
        "user_request = \"Write a Python function to find the nth Fibonacci number.\"\n",
        "initial_input = {\"user_request\": user_request}\n",
        "\n",
        "console.print(f\"[bold cyan]ðŸš€ Kicking off Reflection workflow for request:[/bold cyan] '{user_request}'\\n\")\n",
        "\n",
        "# Corrected: This loop correctly captures the final, fully-populated state\n",
        "final_state = None\n",
        "for state_update in reflection_app.stream(initial_input, stream_mode=\"values\"):\n",
        "    final_state = state_update\n",
        "\n",
        "console.print(\"\\n[bold green]âœ… Reflection workflow complete![/bold green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analysis-what",
      "metadata": {
        "id": "analysis-what"
      },
      "source": [
        "### Step 3.2: Analyzing the 'Before and After'\n",
        "\n",
        "**What we are going to do:**\n",
        "This is the moment of truth. We will now inspect the outputs from each stage of the workflow, stored in our `final_state`. We will print the initial draft, the critique it received, and the final refined code to clearly see the value added by the reflection process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "analysis-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "analysis-code",
        "outputId": "5c5c38a2-53fb-4fa0-aa4a-bb14a198139a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2305295252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if final_state is available and has the expected keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'draft'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'critique'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'refined_code'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- ### Initial Draft ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**Explanation:** {final_state['draft']['explanation']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Use rich's Syntax for proper code highlighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_state' is not defined"
          ]
        }
      ],
      "source": [
        "# Check if final_state is available and has the expected keys\n",
        "if final_state and 'draft' in final_state and 'critique' in final_state and 'refined_code' in final_state:\n",
        "    console.print(Markdown(\"--- ### Initial Draft ---\"))\n",
        "    console.print(Markdown(f\"**Explanation:** {final_state['draft']['explanation']}\"))\n",
        "    # Use rich's Syntax for proper code highlighting\n",
        "    console.print(Syntax(final_state['draft']['code'], \"python\", theme=\"monokai\", line_numbers=True))\n",
        "\n",
        "    console.print(Markdown(\"\\n--- ### Critique ---\"))\n",
        "    console.print(Markdown(f\"**Summary:** {final_state['critique']['critique_summary']}\"))\n",
        "    console.print(Markdown(f\"**Improvements Suggested:**\"))\n",
        "    for improvement in final_state['critique']['suggested_improvements']:\n",
        "        console.print(Markdown(f\"- {improvement}\"))\n",
        "\n",
        "    console.print(Markdown(\"\\n--- ### Final Refined Code ---\"))\n",
        "    console.print(Markdown(f\"**Refinement Summary:** {final_state['refined_code']['refinement_summary']}\"))\n",
        "    console.print(Syntax(final_state['refined_code']['refined_code'], \"python\", theme=\"monokai\", line_numbers=True))\n",
        "else:\n",
        "    console.print(\"[bold red]Error: The `final_state` is not available or is incomplete. Please check the execution of the previous cells.[/bold red]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analysis-discuss",
      "metadata": {
        "id": "analysis-discuss"
      },
      "source": [
        "**Discussion of the Output:**\n",
        "The results are a perfect illustration of the power of reflection.\n",
        "\n",
        "1.  The **Initial Draft** likely produced a simple, recursive solution. While correct, this approach is notoriously inefficient due to re-calculating the same values repeatedly, leading to exponential time complexity.\n",
        "2.  The **Critique** correctly identified this major flaw. The LLM, in its 'critic' role, pointed out the inefficiency and suggested a more optimal, iterative approach to avoid redundant calculations.\n",
        "3.  The **Final Refined Code** successfully implemented the critique. It replaced the slow recursive function with a much faster iterative solution that uses a loop and two variables to keep track of the sequence.\n",
        "\n",
        "This is a non-trivial improvement. The agent didn't just fix a typo; it fundamentally changed its algorithm for a more robust and scalable solution. This is the value of the reflection pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-what",
      "metadata": {
        "id": "eval-what"
      },
      "source": [
        "### Step 3.3: Quantitative Evaluation (LLM-as-a-Judge)\n",
        "\n",
        "**What we are going to do:**\n",
        "To formalize our analysis, we will use another LLM as an impartial 'judge' to score the quality of the initial draft versus the final code. This provides a more objective measure of the improvement gained through reflection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval-code",
      "metadata": {
        "id": "eval-code"
      },
      "outputs": [],
      "source": [
        "class CodeEvaluation(BaseModel):\n",
        "    \"\"\"Schema for evaluating a piece of code.\"\"\"\n",
        "    correctness_score: int = Field(description=\"Score from 1-10 on whether the code is logically correct.\")\n",
        "    efficiency_score: int = Field(description=\"Score from 1-10 on the code's algorithmic efficiency.\")\n",
        "    style_score: int = Field(description=\"Score from 1-10 on code style and readability (PEP 8). \")\n",
        "    justification: str = Field(description=\"A brief justification for the scores.\")\n",
        "\n",
        "judge_llm = llm.with_structured_output(CodeEvaluation)\n",
        "\n",
        "def evaluate_code(code_to_evaluate: str):\n",
        "    prompt = f\"\"\"You are an expert judge of Python code. Evaluate the following function on a scale of 1-10 for correctness, efficiency, and style. Provide a brief justification.\n",
        "\n",
        "    Code:\n",
        "    ```python\n",
        "    {code_to_evaluate}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    return judge_llm.invoke(prompt)\n",
        "\n",
        "if final_state and 'draft' in final_state and 'refined_code' in final_state:\n",
        "    console.print(\"--- Evaluating Initial Draft ---\")\n",
        "    initial_draft_evaluation = evaluate_code(final_state['draft']['code'])\n",
        "    console.print(initial_draft_evaluation.model_dump()) # Corrected: use .model_dump()\n",
        "\n",
        "    console.print(\"\\n--- Evaluating Refined Code ---\")\n",
        "    refined_code_evaluation = evaluate_code(final_state['refined_code']['refined_code'])\n",
        "    console.print(refined_code_evaluation.model_dump()) # Corrected: use .model_dump()\n",
        "else:\n",
        "    console.print(\"[bold red]Error: Cannot perform evaluation because the `final_state` is incomplete.[/bold red]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-discuss",
      "metadata": {
        "id": "eval-discuss"
      },
      "source": [
        "**Discussion of the Output:**\n",
        "The LLM-as-a-Judge evaluation provides quantitative evidence of the reflection pattern's success. The initial draft likely received a high score for correctness but a very low score for efficiency. In contrast, the refined code would have scored highly on both correctness and efficiency. This automated, scored evaluation confirms that the reflection process didn't just change the codeâ€”it demonstrably *improved* it in a measurable way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have successfully built, executed, and evaluated a complete, end-to-end agent using the **Reflection** architecture with Nebius AI Studio models. We have seen firsthand how this simple yet powerful pattern can transform a basic LLM generator into a more sophisticated and reliable problem-solver.\n",
        "\n",
        "By structuring the process into distinct **Generate**, **Critique**, and **Refine** steps and orchestrating them with LangGraph, we created a robust system that can identify and correct its own significant flaws. The tangible improvementâ€”from an inefficient recursive solution to an optimal iterative oneâ€”demonstrates that reflection is a foundational technique for moving beyond trivial agentic tasks and building AI systems that exhibit a deeper level of quality and deliberation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-agents-architectures (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}